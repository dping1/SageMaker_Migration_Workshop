{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SageMaker XGboost for classification or regression\n",
    "\n",
    "This notebook provides instruction on how to use SageMaker built-in algorithm XGboost to solve classification or regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create working folder\n",
    "\n",
    "First, we will create a working folder called **MY_PROJECT** and copy your source code from step 2 into the working folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "DIRECTORY=MY_PROJECT\n",
    "\n",
    "target_dir=$DIRECTORY\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n",
    "\n",
    "SOURCE_DIR=MY_PROJECT\n",
    "\n",
    "cp -a ../step-2/$SOURCE_DIR/* ./$DIRECTORY/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Job_Launcher.ipynb\n",
    "\n",
    "Next let's modify the job_launcher.ipynb file in the working folder to use SageMaker XGboost algorithm for training and hosting by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modify training dataset\n",
    "\n",
    "SageMaker XGboost takes data in CSV and libsvm format for training and inference.  For CSV format, the SageMaker XGboost algorithm requires the label to be the first column.\n",
    "\n",
    "If your current training dataset does not have label in the first column, you can use the follow code sample to change the order of columns in your local data directory and save it back. You can also use your code to make this change.  Please note, you will need to run the dataset modification code before the section called **Upload data to S3**.  So add a new section/cell for this.  \n",
    "\n",
    "```ruby\n",
    "import pandas\n",
    "\n",
    "train_file_name = '<name of training data file>' \n",
    "local_training_data_path = \"./data/train/\" + train_file_name\n",
    "\n",
    "train_df = pandas.read_csv(local_training_data_path)\n",
    "train_df_xg =  train_df['<name of label column>'] + train_df.columns[:-1].tolist()]\n",
    "train_df_xg.to_csv(local_training_data_path, index=False, header=False)\n",
    "```\n",
    "\n",
    "SageMaker takes validation/test data in a separate file. If you have data in single file, split the data into training and validation data file.  You can use the following code sample to split the dataset. You can also use your code to make the change.\n",
    "\n",
    "```ruby\n",
    "split_ratio = 0.7\n",
    "train_data_size = int(train_df_xg.shape[0] * split_ratio)\n",
    "\n",
    "train_data = train_df_xg.iloc[:train_data_size, :]\n",
    "validation_data = train_df_xg.iloc[train_data_size:, :]\n",
    "```\n",
    "\n",
    "Finally, we want to save the modified dataset back to the data directory. SageMaker XGboost does to take index or header columns, so when saving the data back, we don't want to include index or header info. \n",
    "\n",
    "\n",
    "```ruby\n",
    "validation_file_name ='<name of the validation data file>'\n",
    "local_validation_data_path = \"./data/validation/\" + validation_file_name\n",
    "train_data.to_csv(local_training_data_path, index=False, header=False)\n",
    "validation_data.to_csv(local_validation_data_path, index=False, header=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace training code with XGboost Estimator \n",
    "\n",
    "Now we have the data ready, we can use XGboost Estimator to kick off the training.  Before we start with that, we need to clean up some cells that are no longer needed.  Remove all cells starting with the section called **Configure hyperparamters**.\n",
    "\n",
    "After all the cells are cleared, follow the steps below to continue\n",
    "\n",
    "1. First we need find out the container for the XGboost algorithm. Copy and paste the following code into a new cell.  \n",
    "\n",
    "\n",
    "```ruby\n",
    "#Get the container name for the XGboost algorithm\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "```\n",
    "\n",
    "2. XGboost supports the default libsvm and CSV format. If you use csv format, you need to specify the content type correctly. Add the following code to set up the data channel\n",
    "\n",
    "```ruby\n",
    "\n",
    "s3_inp_train = sagemaker.session.s3_input(s3_data=s3_input_train, content_type=\"text/csv\")\n",
    "s3_inp_validation = sagemaker.session.s3_input(s3_data=s3_input_validation, content_type=\"text/csv\")\n",
    "\n",
    "data_channel = {\"train\":s3_inp_train, \"validation\":s3_inp_validation}\n",
    "\n",
    "```\n",
    "\n",
    "2. Next we configure hyperamaters for XGboost algorithm and kick off the training job. Copy and paste the following code in a new cell. Make changes to any hyperparameters and other configuration as needed. It will take a few minutes to spin up the training instances to start training. Change the `<objective value>` for depending on the type of problem. See some of common ones below.\n",
    "\n",
    "    `reg:linear`: linear regression\n",
    "\n",
    "    `reg:logistic`: logistic regression\n",
    "\n",
    "    `binary:logistic`: logistic regression for binary classification, output probability\n",
    "\n",
    "    You can see a complete list of hyperparameter [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)\n",
    "\n",
    "\n",
    "```ruby\n",
    "# Setting up estimator and start the training job\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_s3,\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='<objective value>',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit(data_channel)\n",
    "```\n",
    "\n",
    "3. After the training is completed, let's host the trained model and use it for online predicting.  Copy and paste the following code into a new cell.  It will take a few minutes to spin up the hosting instance\n",
    "\n",
    "```ruby\n",
    "# Deploy trained model to a endpoint \n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')\n",
    "```\n",
    "\n",
    "4. After the hosting environment is ready, we are ready to make inference call.  Copy and past the following code into a new cell.  Make any modification as needed.  Make sure the test data does not contain the label column.\n",
    "\n",
    "```ruby\n",
    "# Making inference call against the newly created endpoint\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None\n",
    "\n",
    "test_data_path = '<test data path>'\n",
    "test_data = pandas.read_csv(test_data_path, index=False, header=False)\n",
    "\n",
    "predictions = xgb_predictor.predict(test_data.as_matrix())\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
