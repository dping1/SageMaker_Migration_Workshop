{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Model hosting using a custom container\n",
    "\n",
    "In this step, we will host your model behind a SageMaker endpoint.  Depending on the format of the model, you could build your own container to host the model or host the model using a built-in container such as the SageMaker TensorFlow container.  First, let's create a working folder called **MY_PROJECT** and copy over some boilerplate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "DIRECTORY=MY_PROJECT\n",
    "\n",
    "target_dir=$DIRECTORY\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n",
    "target_dir=$DIRECTORY/test_data\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n",
    "\n",
    "cp -r ./container ./$DIRECTORY/\n",
    "cp run_docker.ipynb ./$DIRECTORY/run_docker.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can build a separate container for serving the model, or create a single container for both serving and hosting.  For the purpose of demonstrating how the serving process is different from the training, we will create a separate container here.\n",
    "\n",
    "Open up the container folder inside the MY_PROJECT folder, similar to the training container folder and file structure, you will see the following folders and files\n",
    " - **code_base**:  this folder contains the files needed to run a flask server and inference code. The content in this folder will be copy to the working folder when the Dockfile is built.  You will see the following files in the **code_base** directory\n",
    "\n",
    "     - **nginx.conf** contains configuration details for running nginx server. There is no need to change this file\n",
    "     - **wsgi.py** instantiates the flask server which is implemented by the predictor.py file\n",
    "     - **predictor.py** is the file that implements a flask server to do inferences. It's the file that you will modify to implement the scoring for your own algorithm. Specifically, you will need to make the following changes:\n",
    "        - import any packages as needed.  e.g. from sklearn.externals import joblib\n",
    "        - enter a file name for the **model_name** variable. This is the model file to be loaded.\n",
    "        - modify the **ScoringService->get_model()** function to load the model\n",
    "        - modify the **ScoringService->transformation()** to process the input data as needed. You might not need to modify this function if the existing code already meets your needs.\n",
    "     - **serve** implements the scoring service shell. You don't necessarily need to modify it for various algorithms. It starts nginx and gunicorn with the correct configurations and then simply waits until gunicorn exits\n",
    " - **Dockerfile**:  This is the Dockerfile used to build docker image.  Make changes as needed such as installing additional packages.\n",
    " - **build_and_push.sh**:  This shell script builds the docker image and push to your AWS ECR. You will need to change the image name to something unique \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Build custom container\n",
    "\n",
    "Now you are ready to build the container. Open the **build_and_push.sh** file and change the name of the image.  Run **build_and_push.sh** in a terminal by typing **sh build_and_push.sh** to build a container and push to ECR.  Make sure you are in the right working directory below.\n",
    "\n",
    "`/home/ec2-user/SageMaker/SageMaker-Migration-Workshop/step-3/MY_PROJECT/container`\n",
    "\n",
    "You can list the docker images after it is built by typing the command below\n",
    "\n",
    "`docker images`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local docker image testing\n",
    "\n",
    "First, we will download saved model from S3 bucket to local folder structure created for testing docker image locally.\n",
    "\n",
    "Enter a name for the modelpath for the model artifact in S3 before running the cell.  You can find the path of the model in the **SageMaker console->training jobs**. Select the training job and you can find the path to the model artifact under the **Output** section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# copy model from S3 to local directory for testing\n",
    "modelname=model.tar.gz\n",
    "modelpath=s3://sagemaker-demo-dyping/timeseries-rf/models/timeseries-rf-2019-01-11-03-26-06-475/output/model.tar.gz\n",
    "    \n",
    "aws s3 cp $modelpath $(pwd)/MY_PROJECT/container/opt/ml/model/$modelname\n",
    "\n",
    "cd $(pwd)/MY_PROJECT/container/opt/ml/model/\n",
    "tar -xvf $modelname\n",
    "rm $modelname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Run docker image locally for testing.\n",
    "\n",
    "- Run the following command in a terminal window. Make sure you provide a name for the serving docker image.  This starts the docker container locally on your notebook instance.\n",
    "\n",
    "\n",
    "`cd /home/ec2-user/SageMaker/SageMaker-Migration-Workshop/step-3`\n",
    "\n",
    "`image=<image name>`\n",
    "  \n",
    "`docker run -v $(pwd)/MY_PROJECT/container/opt/ml:/opt/ml -p 8080:8080 --rm ${image} serve`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Invoke locally running Docker container\n",
    "After the docker image is running in a separate notebook, let's call the docker container.  \n",
    "\n",
    "1. copy the test file to the test_data directory\n",
    "2. Change the payload to the name of the test file and specify the content type (e.g. text/csv). \n",
    "\n",
    "You can status in the terminal window where the docker container is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "cd MY_PROJECT/test_data/\n",
    "\n",
    "payload=test.csv\n",
    "content=text/csv\n",
    "\n",
    "curl --data-binary @${payload} -H \"Content-Type: ${content}\" -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create model and endpoint in SageMaker\n",
    "\n",
    "Now the image has been created and locally tested, we will now create a model in SageMaker and host it behind an API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session=sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Get the container image url\n",
    "\n",
    "First we need to create a name for the model.  Make sure the name is unique.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_tag = \"timeseries-rf-serve\"\n",
    "account = sagemaker_session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, image_tag)\n",
    "model_name = \"timeseries-model-1\"\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Create Model\n",
    "\n",
    "To create a model in SageMaker, we need 2 pieces of information\n",
    "\n",
    "1. image url for the serving container\n",
    "2. Path to the model artifact in S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "\n",
    "# name of the training job that create the model artifacts\n",
    "job_name =\"<name of training job>\"\n",
    "job_name=\"timeseries-rf-2019-01-11-03-26-06-475\"\n",
    "\n",
    "#you can use the training job name to look up the S3 location for the saved model artifact. \n",
    "#You can also find that information in the management console\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': image,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "#Create the model\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Create Endpoint Configuration\n",
    "\n",
    "Next we need to create an endpoint configuration for hosting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"<name of endpoint config>\"\n",
    "endpoint_config_name = \"timeseries-rf-config\"\n",
    "print(endpoint_config_name)\n",
    "\n",
    "\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Create Endpoint\n",
    "\n",
    "Finally, we can now create the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = \"<name of the endpoint>\"\n",
    "endpoint_name = \"timeseries-rf-api-1\"\n",
    "\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test endpoint\n",
    "\n",
    "First load the test data as payload. Modify the code as needed depending on whether "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_name = 'MY_PROJECT/test_data/test.csv' #customize to your test file\n",
    "\n",
    "#input_df = pd.read_csv(file_name, index_col=None, header=None)\n",
    "\n",
    "input_df = pd.read_csv(file_name)\n",
    "\n",
    "input_df=input_df.set_index('timestamp_loc')\n",
    "\n",
    "#payload = input_df.to_csv(index=None, header=None)\n",
    "payload = input_df.to_csv()\n",
    "\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next invoke the endpoint with payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "result = result.decode(\"utf-8\")\n",
    "result = result.split(',')\n",
    "#label = payload.strip(' ').split()[0]\n",
    "#print ('Input: ',label,'\\nPrediction: ', result[0])\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "model_name = 'credit-model-2018-12-20-17-46-53-438-model'\n",
    "input_location = 's3://sagemaker-demo-dyping/sk-lab/batch/test1.csv'\n",
    "output_location = 's3://sagemaker-demo-dyping/sk-lab/batch/output/'\n",
    "\n",
    "transformer =sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name='Batch-Transform',\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    output_path=output_location\n",
    "    )\n",
    "# To start a transform job:\n",
    "transformer.transform(input_location, content_type='text/csv', split_type='Line')\n",
    "# Then wait until transform job is completed\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
