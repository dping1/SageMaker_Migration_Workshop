{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Training with SagMaker SDK\n",
    "\n",
    "In this step, we will make some changes to your existing source code to work with the SageMaker training service.  While using SageMaker training service is not mandatory, there are many benefits of using SageMaker training service such as on-demand infrastructure provisioning, training job tracking, model tracking, and model deployment.\n",
    "\n",
    "First, let's get some working folders and boilerplate files created. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "DIRECTORY=MY_PROJECT\n",
    "\n",
    "target_dir=$DIRECTORY\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n",
    "target_dir=$DIRECTORY/src\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n",
    "target_dir=$DIRECTORY/data\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n",
    "target_dir=$DIRECTORY/data/train\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n",
    "target_dir=$DIRECTORY/data/validation\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n",
    "target_dir=$DIRECTORY/data/test\n",
    "if [ ! -d \"$target_dir\" ]; then\n",
    "    mkdir $target_dir\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, you will see some folders got created. These folders are created to help with training using SageMaker.\n",
    "\n",
    " - **MY_PROJECT**:  This is the main working folder for this step\n",
    " - **MY_PROJECT/src**:  All your source codes from step 1 will be copied here.  \n",
    " - **MY_PROJECT/data/train**: This is where the training dataset should be stored.\n",
    " - **MY_PROJECT/data/validation**: This is where the validation dataset should be stored\n",
    " - **MY_PROJECT/data/test**:  This is where the test dataset should be stored\n",
    " \n",
    "Next, we need to copy some additional files into **MY_PROJECT** folder based on the ML framework you use.\n",
    "\n",
    "In this Workshop, we currently provide support for either TensorFlow based model training or Scit-Learn based model training.  \n",
    "\n",
    "SageMaker trains ML model using docker container and it provides default training docker images for many ML framework including TensorFlow, MXNet, Chainer, Pytorch, and Scit-learn.  \n",
    "\n",
    "In this workshop, we will use SageMaker TensorFlow container for model training.  And for Scit-learn, we will build your own training container to demonstrate the flexibility of SageMaker in supporting different model training requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Uncomment line below if your framework is TensorFlow/Keras\n",
    "#ML_FRAMEWORK=TF  \n",
    "\n",
    "# Uncomment line below if your ML framework is scit-learn\n",
    "ML_FRAMEWORK=SK\n",
    "\n",
    "SOURCE_DIR=MY_PROJECT\n",
    "\n",
    "DIRECTORY=MY_PROJECT\n",
    "\n",
    "if [ $ML_FRAMEWORK == 'TF' ]; then\n",
    "    cp training_inputs.py ./$DIRECTORY/training_inputs.py\n",
    "    cp job_launcher_tf.ipynb ./$DIRECTORY/job_launcher.ipynb\n",
    "    cp -a ../step-1/$SOURCE_DIR/* ./$DIRECTORY/src\n",
    "fi\n",
    "    \n",
    "if [ $ML_FRAMEWORK == 'SK' ]; then\n",
    "    cp job_launcher_sk.ipynb ./$DIRECTORY/job_launcher.ipynb\n",
    "    cp -r ./container ./$DIRECTORY/\n",
    "    cp -a ../step-1/$SOURCE_DIR/* ./$DIRECTORY/src\n",
    "fi\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to remove the **MY_PROJECT** folder for any reason, you can run the following command to do that.  \n",
    "\n",
    "Please note, all content in the **MY_PROJECT** folder will be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Remove # sign in the line below to execute the command\n",
    "#rm -r MY_PROJECT   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction For Scit-Learn based script\n",
    "\n",
    "For Scit-Learn based algorithm, we will build a docker container for training.  Please note that SageMaker also provides a SKLearn Estimator which can be used for training and hosting scit-learn model.\n",
    "\n",
    "In the **MY_PROJECT** folder, you should see a **container** folder and following folders and files inside it:\n",
    "\n",
    " - **code_base**: This is where you will have your training script and all its dependency files. The content in this folder will be copied to the docker container image during container build step.  \n",
    " \n",
    " - **Dockerfile**: This is the configuration file for building a docker image. You should be able to use this file as is.\n",
    " \n",
    " - **build_and_push.sh**: This shell script is for building the docker image and push to your ECR repo. You will only need to change the image name to reflect the name you want to use\n",
    " \n",
    " - **local_test.ipynb**:  This notebook contains instruction on how to perform local testing of the docker image.\n",
    " \n",
    " - **opt**: This is folder structure created to mirror the folder structure when the container is pushed to SageMaker backend for training. We will use this folder structure to help with local testing. You can copy training data, validation data, testing data to the matching folders, and modify hyperparameter.json file in the config folder.  Your training script will be using files in these folders for model training and saving models\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow the steps below to make the changes and build the container**\n",
    "\n",
    "1. Separate the code for data exploration and data processing from the model training code.\n",
    "\n",
    "2. Split the dataset into separate training and validation files. If you plan to split the dataset in your training code, then you can keep a single file. \n",
    "\n",
    "3. Move the training data, validation data to the **train** and **validation** folders under the **data** directory respectively.\n",
    "\n",
    "4. Port over your model training code to the **train** file in the **code_base** directory. The **train** file will be invoked by the container when the training job is kicked off.  \n",
    "\n",
    "    You will see the following code blocks in the **train** file for standard directories that would exist in a container.  \n",
    "\n",
    "    \n",
    ">prefix = '/opt/ml/'  \n",
    ">input_path = prefix + 'input/data'   \n",
    ">output_path = os.path.join(prefix, 'output')  \n",
    ">model_path = os.path.join(prefix, 'model')   \n",
    ">param_path = os.path.join(prefix, 'input/config/hyperparameters.json')  \n",
    "\n",
    ">channel_name='train'   \n",
    ">training_path = os.path.join(input_path, channel_name)   \n",
    ">channel_name='validation'   \n",
    ">validation_path = os.path.join(input_path, channel_name)   \n",
    "\n",
    "\n",
    "   - **training_path** is where the training dataset will be found, \n",
    "   - **validation_path** is where the validation dataset will be found. \n",
    "   - **param_path** points to the location of **hyperparameters.json** file, \n",
    "   - **model_path** is where you want to save your model artifacts.  \n",
    "\n",
    "Modify **train** file to add the library import, training loop, model evaluation, and model saving code.  You will find various placeholders (e.g. **#BEGIN - ADD YOUR LIBRARY IMPORT BELOW**) in the file to guide you on where to add your custom code.\n",
    "\n",
    "5. Test the script before we build the container\n",
    "    - copy your training data set file and validation data set files to following directories respectively\n",
    "       - MY_PROJECT/container/opt/ml/input/data/train\n",
    "       - MY_PROJECT/container/opt/ml/input/data/test\n",
    "    - Uncomment the line below in the **train** file to use the local directory.  Make sure the comment out this line again before next step.\n",
    "    >#prefix = '../opt/ml/' #for local testing without using container\n",
    "    \n",
    "    - Remove any python packages not required for model training.  This will help reduce the size of docker container when we build it in the next step\n",
    "       \n",
    "    - Inside a terminal, run the train file by typing **python train** in the directory where **train** file resides.  The working directory should be\n",
    "    \n",
    "     `/home/ec2-user/SageMaker/SageMaker-Migration-Workshop/step-2/MY_PROJECT/container/code_base`\n",
    "    \n",
    "    \n",
    "6. Now you are ready to build the container. Open the **build_and_push.sh** file and change the name of the image.  Run **build_and_push.sh** in a terminal by typing **sh build_and_push.sh** to build a container and push to ECR.  Make sure you are in the right working directory below\n",
    "\n",
    "     `/home/ec2-user/SageMaker/SageMaker-Migration-Workshop/step-2/MY_PROJECT/container` \n",
    "\n",
    "     You can list the docker images after it is built by typing the command below\n",
    "\n",
    "     `docker images`\n",
    "\n",
    "\n",
    "7. Open **local_test.ipynb** to perform local testing and ensure the train script and other dependencies are working correctly.  This is optional, but highly recommended, step. If you want to skip this and go to the next step for SageMaker training. Follow the steps below to perform the testing.  If any dependency packages are missing.  Modify **Dockerfile** to add additional packages and repeat step 6 above. If any packages are not needed for the model training, consider removing them to reduce the size of the docker image\n",
    "\n",
    "\n",
    "8. Open **job_launcher.ipynb**, and follow the instructions to launch a training job in SageMaker using the custom docker image we just built and tested.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction for TensorFlow based script\n",
    "\n",
    "For TensorFlow based algorithm, we will use the SageMaker TensorFlow Estimator for training\n",
    "\n",
    "In the **MY_PROJECT** folder, you will see the following additional files\n",
    "\n",
    "   - **job_launcher.ipynb** this notebook launches the training job using SageMaker TensorFlow Estimator\n",
    "   - **training_inputs.py** this module has a function for retrieving a range of parameters values to be used by the training code\n",
    "   \n",
    "Follow the following steps to make changes and train the model\n",
    "\n",
    "### 1. Prepare training script\n",
    "\n",
    "First you will need to modify your training script. You will need to have your main training script as a `.py` file. If you have a `.ipynb` file, export it to an executable script using the **Export Notebook as..** feature under the **File** menu.\n",
    "\n",
    "Upload the `.py` file back into the src directory in the working folder. You will need this `.py` file for SageMaker training later.\n",
    "\n",
    "Now open your main training `.py` script and add the lines in the cell below to your the **main** function in your script or at he beginning of the scrpt. During training, the training code will be provided with information on model directory, training data directory, and validation data directory in the form of command line parameters. The code snippet below will help retrieve those parameters. You can provide your code for retrieving these parameters.\n",
    "\n",
    "```ruby\n",
    "from training_inputs import parse_args\n",
    "\n",
    "args, unknown = parse_args()\n",
    "\n",
    "SM_MODEL_DIR = args.model_dir\n",
    "SM_CHANNEL_TRAIN = args.train\n",
    "SM_CHANNEL_VALIDATION = args.validation\n",
    "```\n",
    "\n",
    "### 2. Modify job_launcher.ipynb\n",
    "\n",
    "Open the **job_launcher.ipynb** file, and follow its instruction to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
